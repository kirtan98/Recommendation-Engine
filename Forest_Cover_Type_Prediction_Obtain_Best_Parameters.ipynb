{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Lm9Q4JyEcY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycWpXlzgzmBA"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/forest-cover-type-prediction/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV7sa1SYDJNa"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHV8dAJkuqYv",
        "outputId": "355aa2ed-fdfa-4038-8b98-c8c349337226"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'Elevation', 'Aspect', 'Slope',\n",
              "       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
              "       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
              "       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n",
              "       'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
              "       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
              "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
              "       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
              "       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
              "       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
              "       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
              "       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
              "       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
              "       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
              "       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40',\n",
              "       'Cover_Type'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Make Copy of train_data data\n",
        "copy_train_data = train_data\n",
        "copy_train_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j99jcRQIuqcJ"
      },
      "outputs": [],
      "source": [
        "# Mean hillshade of the hillshade at 9AM, Noon, and 3PM (0-255)\n",
        "copy_train_data['Hillshade_mean'] = (copy_train_data['Hillshade_9am'] +\n",
        "                                     copy_train_data['Hillshade_Noon'] +\n",
        "                                     copy_train_data['Hillshade_3pm'])/3\n",
        "\n",
        "# 9AM, Noon, 3PM hillshade squared\n",
        "copy_train_data['Hillshade_9am_sq'] = np.square(copy_train_data['Hillshade_9am'])\n",
        "copy_train_data['Hillshade_Noon_sq'] = np.square(copy_train_data['Hillshade_Noon'])\n",
        "copy_train_data['Hillshade_3pm_sq'] = np.square(copy_train_data['Hillshade_3pm'])\n",
        "\n",
        "# interaction_9amnoon\tProduct of hillshades at 9AM and Noon\n",
        "# interaction_noon3pm\tProduct of hillshades at Noon and 3PM\n",
        "# interaction_9am3pm\tProduct of hillshades at 9AM and 3PM\n",
        "\n",
        "copy_train_data['interaction_9amnoon'] = np.multiply(copy_train_data['Hillshade_9am'], copy_train_data['Hillshade_Noon'])\n",
        "copy_train_data['interaction_noon3pm'] = np.multiply(copy_train_data['Hillshade_Noon'], copy_train_data['Hillshade_3pm'])\n",
        "copy_train_data['interaction_9am3pm'] = np.multiply(copy_train_data['Hillshade_3pm'], copy_train_data['Hillshade_9am'])\n",
        "\n",
        "copy_train_data.drop(['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAfch4xxuspW"
      },
      "outputs": [],
      "source": [
        "# Square root of the sum of the squared horizontal & vertical distances to water\n",
        "sum_of_squared_distances = copy_train_data['Horizontal_Distance_To_Hydrology']**2 + copy_train_data['Vertical_Distance_To_Hydrology']**2\n",
        "copy_train_data['Euclidean_Distance_To_Hydrology'] = np.sqrt(sum_of_squared_distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQDCzyKjusrr"
      },
      "outputs": [],
      "source": [
        "# Logarithm of elevation\n",
        "copy_train_data['log_elevation'] = np.log(copy_train_data['Elevation'])\n",
        "\n",
        "# cosine_slope\tThe cosine of the slope, used to partially model the relationships between hillshade\n",
        "copy_train_data['cosine_slope'] = np.cos(np.radians(copy_train_data['Slope']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH8J4dExus7c"
      },
      "outputs": [],
      "source": [
        "X = copy_train_data.drop(['Id', 'Soil_Type7', 'Soil_Type15', 'Cover_Type'], axis=1)\n",
        "y = copy_train_data['Cover_Type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehn1Rd8iH058",
        "outputId": "573ae319-9a8b-427f-a869-f80a3676d038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "{'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "LR Accuracy: \n",
            " 0.6820987654320988\n",
            "LR Confusion matrix: \n",
            " [[402  97   5   0  55   4  57]\n",
            " [149 319  23   1 131  26   9]\n",
            " [  0   4 350  86  23 182   0]\n",
            " [  0   0  40 596   0  25   0]\n",
            " [ 17  80  64   0 459  30   0]\n",
            " [  0  18 110  73  59 390   0]\n",
            " [ 67   3   2   0   2   0 578]]\n",
            "LR Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.65      0.64       620\n",
            "           2       0.61      0.48      0.54       658\n",
            "           3       0.59      0.54      0.56       645\n",
            "           4       0.79      0.90      0.84       661\n",
            "           5       0.63      0.71      0.67       650\n",
            "           6       0.59      0.60      0.60       650\n",
            "           7       0.90      0.89      0.89       652\n",
            "\n",
            "    accuracy                           0.68      4536\n",
            "   macro avg       0.68      0.68      0.68      4536\n",
            "weighted avg       0.68      0.68      0.68      4536\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(random_state=42)\n",
        "\n",
        "lr_params = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
        "    'class_weight': ['balanced'],\n",
        "    'solver' : ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
        "    'max_iter': [100, 200, 500]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lr, param_grid=lr_params, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "lr = LogisticRegression(**grid_search.best_params_)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "lr_accuracy = accuracy_score(y_test, y_pred)\n",
        "lr_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "lr_classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print('LR Accuracy: \\n', lr_accuracy)\n",
        "print('LR Confusion matrix: \\n', lr_conf_matrix)\n",
        "print('LR Classification report: \\n', lr_classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbU23_VrH08A",
        "outputId": "03ff2607-2dfb-4158-c9a5-9e3dbb1c8e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "{'bootstrap': True, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
            "RFC Accuracy: \n",
            " 0.8450176366843033\n",
            "RFC Confusion matrix: \n",
            " [[461  94   1   0  21   1  42]\n",
            " [124 427  20   0  68  13   6]\n",
            " [  0   1 496  38   5 105   0]\n",
            " [  0   0   8 649   0   4   0]\n",
            " [  1  18  14   0 615   2   0]\n",
            " [  0   4  67  22   7 550   0]\n",
            " [ 17   0   0   0   0   0 635]]\n",
            "RFC Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.76      0.74      0.75       620\n",
            "           2       0.78      0.65      0.71       658\n",
            "           3       0.82      0.77      0.79       645\n",
            "           4       0.92      0.98      0.95       661\n",
            "           5       0.86      0.95      0.90       650\n",
            "           6       0.81      0.85      0.83       650\n",
            "           7       0.93      0.97      0.95       652\n",
            "\n",
            "    accuracy                           0.85      4536\n",
            "   macro avg       0.84      0.84      0.84      4536\n",
            "weighted avg       0.84      0.85      0.84      4536\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rfc = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rfc_params = {\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_features': ['auto'],\n",
        "    'bootstrap': [True],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'class_weight' : ['balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rfc, param_grid=rfc_params, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "rfc = RandomForestClassifier(**grid_search.best_params_)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rfc.predict(X_test)\n",
        "rfc_accuracy = accuracy_score(y_test, y_pred)\n",
        "rfc_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "rfc_classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print('RFC Accuracy: \\n', rfc_accuracy)\n",
        "print('RFC Confusion matrix: \\n', rfc_conf_matrix)\n",
        "print('RFC Classification report: \\n', rfc_classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJFK7heYH0_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d38d82-74e9-446e-a0e6-92f1d70ca514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "{'bootstrap': True, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
            "EXTR Accuracy: \n",
            " 0.8434744268077602\n",
            "EXTR Confusion matrix: \n",
            " [[453 107   1   0  25   1  33]\n",
            " [121 437  22   0  64  11   3]\n",
            " [  0   1 499  46   3  96   0]\n",
            " [  0   0   7 649   0   5   0]\n",
            " [  3  16  15   0 612   4   0]\n",
            " [  0   3  69  25   5 548   0]\n",
            " [ 21   3   0   0   0   0 628]]\n",
            "EXTR Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.76      0.73      0.74       620\n",
            "           2       0.77      0.66      0.71       658\n",
            "           3       0.81      0.77      0.79       645\n",
            "           4       0.90      0.98      0.94       661\n",
            "           5       0.86      0.94      0.90       650\n",
            "           6       0.82      0.84      0.83       650\n",
            "           7       0.95      0.96      0.95       652\n",
            "\n",
            "    accuracy                           0.84      4536\n",
            "   macro avg       0.84      0.84      0.84      4536\n",
            "weighted avg       0.84      0.84      0.84      4536\n",
            "\n"
          ]
        }
      ],
      "source": [
        "extra_trees = ExtraTreesClassifier(random_state=42)\n",
        "\n",
        "extr_params = {\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
        "    'max_features': ['auto'],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True],\n",
        "    'class_weight' : ['balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=extra_trees, param_grid=extr_params, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "extr = ExtraTreesClassifier(**grid_search.best_params_)\n",
        "extr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = extr.predict(X_test)\n",
        "\n",
        "extr_accuracy = accuracy_score(y_test, y_pred)\n",
        "extr_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "extr_extree_classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print('EXTR Accuracy: \\n', extr_accuracy)\n",
        "print('EXTR Confusion matrix: \\n', extr_conf_matrix)\n",
        "print('EXTR Classification report: \\n', extr_extree_classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjlp96UIus90"
      },
      "outputs": [],
      "source": [
        "dec_trees = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "dec_params = {\n",
        "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=dec_trees, param_grid=dec_params, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "dec = DecisionTreeClassifier(**grid_search.best_params_)\n",
        "dec.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dec.predict(X_test)\n",
        "\n",
        "dec_accuracy = accuracy_score(y_test, y_pred)\n",
        "dec_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "dec_extree_classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print('DEC Accuracy: \\n', dec_accuracy)\n",
        "print('DEC Confusion matrix: \\n', dec_conf_matrix)\n",
        "print('DEC Classification report: \\n', dec_extree_classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vo4eSofbFAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57894ab4-9247-4336-a92a-624710b46c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "{'criterion': 'friedman_mse', 'learning_rate': 0.1, 'max_features': None, 'n_estimators': 500}\n",
            "GB Accuracy: \n",
            " 0.8271604938271605\n",
            "GB Confusion matrix: \n",
            " [[456 114   1   0  16   2  31]\n",
            " [137 418  24   0  64  12   3]\n",
            " [  0   5 490  31   6 112   1]\n",
            " [  0   0  12 642   0   7   0]\n",
            " [  3  25  18   0 602   2   0]\n",
            " [  0  16  90  16   7 521   0]\n",
            " [ 28   1   0   0   0   0 623]]\n",
            "GB Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.73      0.74      0.73       620\n",
            "           2       0.72      0.64      0.68       658\n",
            "           3       0.77      0.76      0.77       645\n",
            "           4       0.93      0.97      0.95       661\n",
            "           5       0.87      0.93      0.90       650\n",
            "           6       0.79      0.80      0.80       650\n",
            "           7       0.95      0.96      0.95       652\n",
            "\n",
            "    accuracy                           0.83      4536\n",
            "   macro avg       0.82      0.83      0.82      4536\n",
            "weighted avg       0.82      0.83      0.83      4536\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "gb_params = {\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'criterion': ['friedman_mse', 'squared_error'],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=gb, param_grid=gb_params, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "gb = GradientBoostingClassifier(**grid_search.best_params_)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "gb_accuracy = accuracy_score(y_test, y_pred)\n",
        "gb_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "gb_extree_classification_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print('GB Accuracy: \\n', gb_accuracy)\n",
        "print('GB Confusion matrix: \\n', gb_conf_matrix)\n",
        "print('GB Classification report: \\n', gb_extree_classification_report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}